{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Define the environment\n",
    "class SimuEnv:\n",
    "    #The defined model as class parameter\n",
    "    # P[s][a][s']\n",
    "    transition_probabilities = [\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], # in s0, if action a0 then proba 0.7 to state s0 and 0.3 to state s1, etc.\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None],\n",
    "        ]\n",
    "    # R[s][a][s']\n",
    "    rewards = [\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]],\n",
    "        ]\n",
    "\n",
    "    possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "    n_states = 3\n",
    "    n_actions = 3\n",
    "    def __init__(self, start_state = 0, discount_rate = 1):\n",
    "        self.start_state = start_state\n",
    "        self.total_reward = 0\n",
    "        self.discount_rate = discount_rate\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        self.total_reward = 0\n",
    "        return self.start_state, 0, False, \"\"\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action not in SimuEnv.possible_actions[self.current_state]:\n",
    "            return 0, 0, True, \"Illegal action for state %d.\" % (self.current_state)\n",
    "        trans_prop = SimuEnv.transition_probabilities[self.current_state][action]\n",
    "        next_state = np.random.choice(3, 1, p = trans_prop)[0]\n",
    "        reward = SimuEnv.rewards[self.current_state][action][next_state]\n",
    "        self.current_state = next_state\n",
    "        self.total_reward = self.discount_rate * self.total_reward + reward\n",
    "        return next_state, reward, False, \"\"\n",
    "\n",
    "def policy_try(state):\n",
    "    return [0, 2, 1][state]\n",
    "\n",
    "def policy_safe(state):\n",
    "    return [0, 0, 1][state]\n",
    "\n",
    "def simulation(policy):\n",
    "    env = SimuEnv(discount_rate=0.95)\n",
    "    obs = env.reset()\n",
    "    op_stack = []\n",
    "    for i in range(10000):\n",
    "        state = obs[0]\n",
    "        action = policy(state)\n",
    "        obs = env.step(action)\n",
    "        op_stack.append((state, action, obs[1], obs[0]))\n",
    "    \n",
    "    #print (op_stack)\n",
    "    return op_stack, env.total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_reward(policy, n_iteration):\n",
    "    avg = 0\n",
    "    for iter in range(n_iteration):\n",
    "        op_stack, total_reward = simulation(policy)\n",
    "        avg = (total_reward + iter * avg) / (iter + 1)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.593528520829864\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_reward(policy_try, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q Value Iteration......\n",
    "Bellman's Equation\n",
    "Q(s, a) = sum (P(s,a,s') * (R(s, a, s') + gama * [max(a') s.t. Q(s', a')] ))\n",
    "\"\"\"\n",
    "def Qiter(model, gama, n_iter):\n",
    "    Q = np.zeros([model.n_states, model.n_actions])\n",
    "    tp = model.transition_probabilities\n",
    "    r = model.rewards\n",
    "    for iter in range(n_iter):\n",
    "        for state in range(model.n_states):\n",
    "            for action in model.possible_actions[state]:\n",
    "                Q[state][action] = sum([tp[state][action][s_state] *\\\n",
    "                                        (r[state][action][s_state] +\\\n",
    "                                         gama *\\\n",
    "                                         max([Q[s_state][action] for action in model.possible_actions[s_state]])) \n",
    "                                        for s_state in range(model.n_states)])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Qiter(SimuEnv, 1, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30920.73009586, 30920.73009586, 30916.78464131],\n",
       "       [30901.00282313,     0.        , 30902.54827768],\n",
       "       [    0.        , 30954.09373222,     0.        ]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(3), 10, p = [0.5, 0.5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q learning with TD(0) and sigma-greedy:\n",
    "Q(s, a) = (1 - a)Q(s, a) + a(r(s,a,s') + gama * max(a') [Q(s', a')])\n",
    "\"\"\"\n",
    "\n",
    "def Qlearning(model, gama, alpha, n_iter, sigma, decay_rate):\n",
    "    n = np.full((model.n_states, model.n_actions), 0)\n",
    "    Q = np.full((model.n_states, model.n_actions), 0.)\n",
    "    Q[1, 1] = -np.inf\n",
    "    Q[2, 0], Q[2, 2] = -np.inf, -np.inf\n",
    "    env = model()\n",
    "    state = 0\n",
    "    pa = model.possible_actions\n",
    "    for iteration in range(n_iter):\n",
    "        action = np.random.choice(pa[state])\n",
    "        obs = env.step(action)\n",
    "        s_state, s_reward, done, info = obs\n",
    "        n[state][action] += 1\n",
    "        Q[state][action] = (1 - alpha)*Q[state][action] + alpha * (s_reward + gama * max([Q[s_state][s_action] for\n",
    "                                                                                         s_action in pa[s_state]]))\n",
    "        alpha = alpha / (1 + iteration * decay_rate / 10.0)\n",
    "    return Q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0004796 ,  0.15982627, -0.27980236],\n",
       "       [ 0.        ,        -inf,  0.        ],\n",
       "       [       -inf,  0.        ,        -inf]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
